{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "378d8b6f",
   "metadata": {},
   "source": [
    "# Lab 4a: Hardware Benchmarking (HW Accelerators Prespective)\n",
    "## Hardware for Machine Learning Course\n",
    "\n",
    "This notebook is to benchmark different HWs from their architecture prespective.\n",
    "Part-1 covers:\n",
    "1. Environment setup, Model and dataset preparation\n",
    "2. CPU performance benchmarking\n",
    "3. GPU performance benchmarking\n",
    "\n",
    "Part-2 covers:\n",
    "1. Model quntization on FP16 and INT8.\n",
    "2. Evaluation and comparison among quantized models.\n",
    "\n",
    "Part-2 covers:\n",
    "1. Model pruning.\n",
    "2. Pruning sparsity vs accuracy trade-offs.\n",
    "\n",
    "The lab will explore how a neural network model perform across different hardware platforms and how they can be optimized for specific deployment scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4612fb",
   "metadata": {},
   "source": [
    "## PART-1: LATENCY BENCHMARKING\n",
    "### ENVIRONMENT SETUP\n",
    "\n",
    "First, we'll set up our environment by importing necessary libraries and checking available hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2247dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the needed libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models, losses, optimizers, metrics\n",
    "import os\n",
    "\n",
    "# Check for accelerator (GPU or TPU)\n",
    "accelerator_found = False\n",
    "using_tpu = False\n",
    "strategy = None\n",
    "\n",
    "# Check for TPU first (specifically for Colab)\n",
    "try:\n",
    "    # For Colab TPU, we need to use the following approach\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()  # Simplified TPU initialization for Colab\n",
    "    print(f\"TPU detected, initializing...\")\n",
    "    \n",
    "    # Create TPU strategy\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    using_tpu = True\n",
    "    accelerator_found = True\n",
    "    print(f\"TPU initialized with {strategy.num_replicas_in_sync} replicas\")\n",
    "    print(f\"TPU type: {tpu.get_master()}\")  # This will show the TPU type\n",
    "except (ValueError, AttributeError, ImportError, Exception) as e:\n",
    "    print(f\"TPU not available: {e}\")\n",
    "    \n",
    "# Check for GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    accelerator_found = True\n",
    "    strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    print(f\"GPU detected: {gpus[0].name}\")\n",
    "else:\n",
    "    print(\"No accelerator detected!\")\n",
    "\n",
    "if not accelerator_found:\n",
    "    raise RuntimeError(\"No accelerator (GPU or TPU) detected! Please choose a GPU or TPU runtime in Colab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb70de1",
   "metadata": {},
   "source": [
    "### MODEL, DATASET PREPARATION, AND BUILDING HELPING FUNCTIONS\n",
    "Now we'll create our model architectures and prepare the CIFAR-10 dataset for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bcb276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A CNN MODEL\n",
    "def create_cnn_model(num_classes=10):\n",
    "    \"\"\"Create a simple CNN model for classification.\"\"\"\n",
    "    model = models.Sequential([\n",
    "        # First convolutional block\n",
    "        layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_and_prepare_cifar10(batch_size=32, train_fraction=0.1, val_fraction=0.1):\n",
    "    \"\"\"\n",
    "    Load and prepare a small portion of CIFAR-10 dataset.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Batch size for datasets\n",
    "        train_fraction: Fraction of training data to use (0.0 to 1.0)\n",
    "        val_fraction: Fraction of training data to use for validation (0.0 to 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        train_ds, val_ds, test_ds: tf.data.Dataset objects\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define preprocessing function\n",
    "    def preprocess(image, label):\n",
    "        # Convert image to float32 and normalize to [0,1]\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "        # Normalize with CIFAR-10 mean and std\n",
    "        mean = tf.constant([0.4914, 0.4822, 0.4465])\n",
    "        std = tf.constant([0.2470, 0.2435, 0.2616])\n",
    "        image = (image - mean) / std\n",
    "        return image, label\n",
    "    \n",
    "    # Load full CIFAR-10 dataset\n",
    "    train_ds_full, test_ds = tfds.load(\n",
    "        'cifar10',\n",
    "        split=['train', 'test'],\n",
    "        as_supervised=True,\n",
    "        batch_size=-1  # Load all data at once for splitting\n",
    "    )\n",
    "    \n",
    "    # Convert to numpy for easier subset creation\n",
    "    train_images, train_labels = tfds.as_numpy(train_ds_full)\n",
    "    test_images, test_labels = tfds.as_numpy(test_ds)\n",
    "    \n",
    "    # Calculate sizes for training subset\n",
    "    total_train = len(train_images)\n",
    "    train_size = int(total_train * train_fraction)\n",
    "    val_size = int(total_train * val_fraction)\n",
    "    \n",
    "    # Randomly shuffle indices\n",
    "    indices = np.random.permutation(total_train)\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:train_size + val_size]\n",
    "    \n",
    "    # Create subsets\n",
    "    train_images_subset = train_images[train_indices]\n",
    "    train_labels_subset = train_labels[train_indices]\n",
    "    val_images = train_images[val_indices]\n",
    "    val_labels = train_labels[val_indices]\n",
    "    \n",
    "    print(f\"Training samples: {len(train_images_subset)}\")\n",
    "    print(f\"Validation samples: {len(val_images)}\")\n",
    "    print(f\"Test samples: {len(test_images)}\")\n",
    "    \n",
    "    # Create tf.data.Dataset objects\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_images_subset, train_labels_subset))\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "    \n",
    "    # Apply preprocessing and batching\n",
    "    train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Batch and prefetch\n",
    "    train_ds = train_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35286b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_strategy(strategy, train_ds, val_ds, num_epochs=5):\n",
    "    \"\"\"Train the model using the specified distribution strategy.\"\"\"\n",
    "    \n",
    "    print(f\"Training with strategy: {strategy}\")\n",
    "    \n",
    "    with strategy.scope():\n",
    "        # Create model\n",
    "        model = create_cnn_model(num_classes=10)\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "            loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Print model summary\n",
    "        model.summary()\n",
    "    \n",
    "    # Initialize timing variables\n",
    "    total_training_time = 0\n",
    "    total_samples_processed = 0\n",
    "    epoch_metrics = []\n",
    "    \n",
    "    # Custom training loop for timing\n",
    "    for epoch in range(num_epochs):\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train for one epoch\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            epochs=1,\n",
    "            validation_data=val_ds,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # End timing\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate samples processed\n",
    "        samples_processed = sum(1 for _ in train_ds.unbatch().batch(1))\n",
    "        \n",
    "        # Update totals\n",
    "        total_training_time += epoch_time\n",
    "        total_samples_processed += samples_processed\n",
    "        \n",
    "        # Calculate samples per second\n",
    "        samples_per_second = samples_processed / epoch_time\n",
    "        \n",
    "        # Store metrics\n",
    "        epoch_metrics.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': history.history['loss'][0],\n",
    "            'train_acc': history.history['accuracy'][0],\n",
    "            'val_loss': history.history['val_loss'][0],\n",
    "            'val_acc': history.history['val_accuracy'][0],\n",
    "            'epoch_time': epoch_time,\n",
    "            'samples_per_second': samples_per_second\n",
    "        })\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {history.history[\"loss\"][0]:.4f}, Train Acc: {history.history[\"accuracy\"][0]:.4f}')\n",
    "        print(f'Val Loss: {history.history[\"val_loss\"][0]:.4f}, Val Acc: {history.history[\"val_accuracy\"][0]:.4f}')\n",
    "        print(f'Epoch Time: {epoch_time:.2f}s, Samples/sec: {samples_per_second:.2f}')\n",
    "        print('-' * 50)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    avg_samples_per_second = total_samples_processed / total_training_time\n",
    "    final_val_acc = epoch_metrics[-1]['val_acc']\n",
    "    \n",
    "    print(f\"\\nOverall Training Summary:\")\n",
    "    print(f\"Total Training Time: {total_training_time:.2f}s\")\n",
    "    print(f\"Average Samples/second: {avg_samples_per_second:.2f}\")\n",
    "    print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "    \n",
    "    return model, total_training_time, avg_samples_per_second, final_val_acc\n",
    "\n",
    "def test_model(model, test_ds):\n",
    "    \"\"\"Test the model.\"\"\"\n",
    "    start_time = time.time()\n",
    "    test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
    "    test_time = time.time() - start_time\n",
    "    \n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "    print(f'Test Time: {test_time:.2f}s')\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def profile_inference(models_dict, test_ds, batch_sizes=[1, 32, 64], num_iterations=100):\n",
    "    \"\"\"\n",
    "    Profile inference performance on CPU and accelerator for different batch sizes.\n",
    "    \n",
    "    Args:\n",
    "        models_dict: Dictionary with keys 'cpu' and 'accelerator' containing trained models\n",
    "        test_ds: tf.data.Dataset for test dataset\n",
    "        batch_sizes: List of batch sizes to profile\n",
    "        num_iterations: Number of inference iterations for accurate timing\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with profiling results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'cpu': {},\n",
    "        'accelerator': {}\n",
    "    }\n",
    "    \n",
    "    # Get a fixed batch from test dataset\n",
    "    for batch in test_ds.take(1):\n",
    "        fixed_inputs, _ = batch\n",
    "    \n",
    "    for device_name, model in models_dict.items():\n",
    "        print(f\"\\nProfiling inference on {device_name.upper()}...\")\n",
    "        \n",
    "        # Determine device to use\n",
    "        if device_name == 'cpu':\n",
    "            device = '/CPU:0'\n",
    "        else:\n",
    "            # For accelerator, check what's available\n",
    "            if tf.config.list_physical_devices('GPU'):\n",
    "                device = '/GPU:0'\n",
    "            elif tf.config.list_physical_devices('TPU'):\n",
    "                device = '/TPU:0'\n",
    "            else:\n",
    "                device = '/CPU:0'\n",
    "                print(f\"  Warning: No accelerator found, using CPU for {device_name} profiling\")\n",
    "        \n",
    "        print(f\"  Using device: {device}\")\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            # Prepare batch\n",
    "            if batch_size <= fixed_inputs.shape[0]:\n",
    "                inputs = fixed_inputs[:batch_size]\n",
    "            else:\n",
    "                # Repeat to create larger batch\n",
    "                repeats = (batch_size + fixed_inputs.shape[0] - 1) // fixed_inputs.shape[0]\n",
    "                inputs = tf.tile(fixed_inputs, [repeats, 1, 1, 1])[:batch_size]\n",
    "            \n",
    "            # Create a tf.function for faster inference (especially on GPU)\n",
    "            @tf.function\n",
    "            def infer_step(x):\n",
    "                return model(x, training=False)\n",
    "            \n",
    "            # Warm-up with explicit device context\n",
    "            with tf.device(device):\n",
    "                for _ in range(10):\n",
    "                    _ = infer_step(inputs)\n",
    "            \n",
    "            # Force synchronization for GPU/TPU by running a small operation\n",
    "            if device != '/CPU:0':\n",
    "                # Create a small dummy tensor and force evaluation\n",
    "                _ = tf.constant(0).numpy() if device == '/CPU:0' else tf.constant(0)\n",
    "            \n",
    "            # Timed inference with explicit device context\n",
    "            total_time_seconds = 0\n",
    "            \n",
    "            # Use time.time() with appropriate synchronization\n",
    "            with tf.device(device):\n",
    "                # For GPU, we need to ensure all operations are complete before/after timing\n",
    "                if device != '/CPU:0':\n",
    "                    # Synchronize by running a small operation and forcing evaluation\n",
    "                    _ = tf.constant(0)\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                for _ in range(num_iterations):\n",
    "                    _ = infer_step(inputs)\n",
    "                \n",
    "                # For GPU/TPU, ensure all operations are complete\n",
    "                if device != '/CPU:0':\n",
    "                    # Force a synchronization by converting a small tensor to numpy\n",
    "                    _ = tf.constant(0).numpy()\n",
    "                \n",
    "                end_time = time.time()\n",
    "                total_time_seconds = end_time - start_time\n",
    "            \n",
    "            total_time_ms = total_time_seconds * 1000\n",
    "            \n",
    "            # Calculate metrics\n",
    "            avg_time_per_batch_ms = total_time_ms / num_iterations\n",
    "            samples_per_second = (batch_size * num_iterations) / total_time_seconds\n",
    "            \n",
    "            results[device_name][batch_size] = {\n",
    "                'avg_inference_time_ms': avg_time_per_batch_ms,\n",
    "                'samples_per_second': samples_per_second,\n",
    "                'total_time_seconds': total_time_seconds,\n",
    "                'num_iterations': num_iterations,\n",
    "                'device_used': device\n",
    "            }\n",
    "            \n",
    "            print(f\"  Batch size {batch_size}:\")\n",
    "            print(f\"    Device: {device}\")\n",
    "            print(f\"    Avg inference time: {avg_time_per_batch_ms:.2f} ms\")\n",
    "            print(f\"    Samples/second: {samples_per_second:.2f}\")\n",
    "    \n",
    "    # Print comparison summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"INFERENCE PROFILING SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Batch Size':<12} {'Metric':<20} {'CPU':<15} {'Accelerator':<15} {'Speedup':<10}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        cpu_time = results['cpu'][batch_size]['avg_inference_time_ms']\n",
    "        acc_time = results['accelerator'][batch_size]['avg_inference_time_ms']\n",
    "        speedup_time = cpu_time / acc_time if acc_time > 0 else float('inf')\n",
    "        \n",
    "        cpu_throughput = results['cpu'][batch_size]['samples_per_second']\n",
    "        acc_throughput = results['accelerator'][batch_size]['samples_per_second']\n",
    "        speedup_throughput = acc_throughput / cpu_throughput if cpu_throughput > 0 else float('inf')\n",
    "        \n",
    "        print(f\"{batch_size:<12} {'Time (ms)':<20} {cpu_time:<15.2f} {acc_time:<15.2f} {speedup_time:<10.2f}x\")\n",
    "        print(f\"{batch_size:<12} {'Samples/sec':<20} {cpu_throughput:<15.2f} {acc_throughput:<15.2f} {speedup_throughput:<10.2f}x\")\n",
    "        print(\"-\"*70)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_inference_results(profile_results):\n",
    "    \"\"\"\n",
    "    Plot inference profiling results comparing CPU vs Accelerator.\n",
    "    \n",
    "    Args:\n",
    "        profile_results: Dictionary returned by profile_inference() function\n",
    "    \"\"\"\n",
    "    batch_sizes = list(profile_results['cpu'].keys())\n",
    "    \n",
    "    # Extract data\n",
    "    cpu_times = [profile_results['cpu'][bs]['avg_inference_time_ms'] for bs in batch_sizes]\n",
    "    acc_times = [profile_results['accelerator'][bs]['avg_inference_time_ms'] for bs in batch_sizes]\n",
    "    \n",
    "    cpu_throughput = [profile_results['cpu'][bs]['samples_per_second'] for bs in batch_sizes]\n",
    "    acc_throughput = [profile_results['accelerator'][bs]['samples_per_second'] for bs in batch_sizes]\n",
    "    \n",
    "    # Calculate speedups\n",
    "    time_speedups = [cpu_times[i] / acc_times[i] for i in range(len(batch_sizes))]\n",
    "    throughput_speedups = [acc_throughput[i] / cpu_throughput[i] for i in range(len(batch_sizes))]\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.suptitle('Inference Performance: CPU vs Accelerator', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Inference Time (bar chart)\n",
    "    x = np.arange(len(batch_sizes))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, cpu_times, width, label='CPU', color='skyblue', edgecolor='navy')\n",
    "    bars2 = ax1.bar(x + width/2, acc_times, width, label='Accelerator', color='lightcoral', edgecolor='darkred')\n",
    "    \n",
    "    ax1.set_xlabel('Batch Size', fontsize=12)\n",
    "    ax1.set_ylabel('Inference Time (ms)', fontsize=12)\n",
    "    ax1.set_title('Average Inference Time per Batch', fontsize=14)\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([f'BS={bs}' for bs in batch_sizes])\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Plot 2: Throughput (bar chart)\n",
    "    bars1 = ax2.bar(x - width/2, cpu_throughput, width, label='CPU', color='skyblue', edgecolor='navy')\n",
    "    bars2 = ax2.bar(x + width/2, acc_throughput, width, label='Accelerator', color='lightcoral', edgecolor='darkred')\n",
    "    \n",
    "    ax2.set_xlabel('Batch Size', fontsize=12)\n",
    "    ax2.set_ylabel('Samples/Second', fontsize=12)\n",
    "    ax2.set_title('Throughput', fontsize=14)\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels([f'BS={bs}' for bs in batch_sizes])\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.0f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed summary table\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DETAILED INFERENCE PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Batch Size':<12} {'Metric':<20} {'CPU':<15} {'Accelerator':<15} {'Speedup':<10}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for i, bs in enumerate(batch_sizes):\n",
    "        print(f\"{bs:<12} {'Time (ms)':<20} {cpu_times[i]:<15.2f} {acc_times[i]:<15.2f} {time_speedups[i]:<10.2f}x\")\n",
    "        print(f\"{bs:<12} {'Samples/sec':<20} {cpu_throughput[i]:<15.0f} {acc_throughput[i]:<15.0f} {throughput_speedups[i]:<10.2f}x\")\n",
    "        print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fee23a",
   "metadata": {},
   "source": [
    "### PREPARING BENCHARKING FOR BOTH THE CPU AND THE GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b420b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "print(\"Loading and preparing CIFAR-10 dataset...\")\n",
    "train_ds, val_ds, test_ds = load_and_prepare_cifar10(\n",
    "    batch_size=32,\n",
    "    train_fraction=0.1,\n",
    "    val_fraction=0.1\n",
    ")\n",
    "num_epochs = 3\n",
    "# Print CIFAR-10 class names\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "print(f\"Classes: {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b01c21",
   "metadata": {},
   "source": [
    "### BENCHMARKING THE CPU TRAINING\n",
    "Measuring baseline performance on CPU before accelerator comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99123463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on CPU\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ON CPU\")\n",
    "print(\"=\"*60)\n",
    "cpu_strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "cpu_model, cpu_time, cpu_samples_per_sec, cpu_val_acc = train_with_strategy(\n",
    "    cpu_strategy, train_ds, val_ds, num_epochs\n",
    ")\n",
    "\n",
    "# Test CPU model\n",
    "print(\"\\nTesting CPU model:\")\n",
    "cpu_test_loss, cpu_test_acc = test_model(cpu_model, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef290f3",
   "metadata": {},
   "source": [
    "### BENCHMARKING THE ACCELERATOR TRAINING \n",
    "Evaluating training speed and throughput on GPU/TPU hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e4ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on accelerator (GPU or TPU)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ON ACCELERATOR\")\n",
    "print(\"=\"*60)\n",
    "accelerator_model, accelerator_time, accelerator_samples_per_sec, accelerator_val_acc = train_with_strategy(\n",
    "    strategy, train_ds, val_ds, num_epochs\n",
    ")\n",
    "\n",
    "# Test accelerator model\n",
    "print(\"\\nTesting Accelerator model:\")\n",
    "accelerator_test_loss, accelerator_test_acc = test_model(accelerator_model, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758e9500",
   "metadata": {},
   "source": [
    "### BENCHMARKING THE CPU AND THE ACCELERATOR INFERENCE\n",
    "Comparing inference latency and throughput across hardware platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f92d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile inference\n",
    "models_for_profiling = {\n",
    "    'cpu': cpu_model,\n",
    "    'accelerator': accelerator_model\n",
    "}\n",
    "profile_results = profile_inference(models_for_profiling, test_ds, batch_sizes=[1, 32, 64], num_iterations=100)\n",
    "\n",
    "# Plot results\n",
    "plot_inference_results(profile_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb40c1",
   "metadata": {},
   "source": [
    "## Part-2: MODEL QUANTIZATION AND PRUNING\n",
    "In this part we will quantize a model then benchmark its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577c5106",
   "metadata": {},
   "source": [
    "### IMPORTING NEEDED LIBRARIES AND BUILDING HELPING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e14082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved CNN model\n",
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Save the accelerator model first\n",
    "accelerator_model.save('accelerator_model.h5')\n",
    "print(\"Accelerator model saved as 'accelerator_model.h5'\")\n",
    "original_model = load_model('accelerator_model.h5')\n",
    "\n",
    "# Since your test_ds is a batched tf.data.Dataset, you need to extract the data\n",
    "print(\"\\nExtracting test data for evaluation...\")\n",
    "\n",
    "# Extract test data from the test_ds dataset\n",
    "x_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for images, labels in test_ds.unbatch():  # Unbatch to get individual samples\n",
    "    x_test_list.append(images.numpy())\n",
    "    y_test_list.append(labels.numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "x_test = np.array(x_test_list)\n",
    "y_test = np.array(y_test_list)\n",
    "\n",
    "print(f\"Test data shape: {x_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "\n",
    "# Similarly, extract training data for representative dataset\n",
    "x_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "# Note: train_ds is also batched, so we need to unbatch it\n",
    "for images, labels in train_ds.unbatch():\n",
    "    x_train_list.append(images.numpy())\n",
    "    y_train_list.append(labels.numpy())\n",
    "\n",
    "x_train = np.array(x_train_list)\n",
    "y_train = np.array(y_train_list)\n",
    "\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(model, x_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"Evaluate model accuracy and size.\"\"\"\n",
    "    # Measure model size\n",
    "    model_size = 0\n",
    "    if hasattr(model, 'weights'):\n",
    "        for weight in model.weights:\n",
    "            model_size += weight.numpy().nbytes\n",
    "    else:\n",
    "        model.save('temp_model.h5')\n",
    "        model_size = os.path.getsize('temp_model.h5')\n",
    "        os.remove('temp_model.h5')\n",
    "\n",
    "    # Evaluate model accuracy\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "    print(f\"{model_name} Evaluation:\")\n",
    "    print(f\"  Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  Model Size: {model_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy * 100,\n",
    "        'model_size_mb': model_size / 1024 / 1024\n",
    "    }\n",
    "\n",
    "# Evaluate original model\n",
    "original_results = evaluate_model(original_model, x_test, y_test, \"Original Model\")\n",
    "\n",
    "# Apply post-training quantization with TensorFlow Lite\n",
    "# First, create a representative dataset for quantization\n",
    "def representative_dataset():\n",
    "    # Use a subset of training data for representative dataset\n",
    "    for i in range(min(5, len(x_train))):\n",
    "        # The input needs to be in the correct shape for the model\n",
    "        # The model expects input shape (1, 32, 32, 3)\n",
    "        yield [x_train[i:i+1].astype(np.float32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81b57b0",
   "metadata": {},
   "source": [
    "### TensorFlow Lite Conversion\n",
    "\n",
    "First, we'll convert our model to TensorFlow Lite format, which is optimized for mobile and edge devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83134337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TFLite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open('original_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# Check the size of the TFLite model\n",
    "tflite_model_size = os.path.getsize('original_model.tflite')\n",
    "print(f\"\\nTFLite model size: {tflite_model_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Quantize the model to float16\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_fp16_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2cbdb3",
   "metadata": {},
   "source": [
    "### Float16 Quantization\n",
    "\n",
    "Next, we'll apply Float16 quantization, which reduces the precision of weights from float32 to float16, potentially reducing model size by up to 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e514fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model\n",
    "with open('quantized_fp16_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_fp16_model)\n",
    "\n",
    "# Check the size of the quantized TFLite model\n",
    "tflite_fp16_model_size = os.path.getsize('quantized_fp16_model.tflite')\n",
    "print(f\"Float16 quantized TFLite model size: {tflite_fp16_model_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Size reduction: {(tflite_model_size / tflite_fp16_model_size):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d91c92",
   "metadata": {},
   "source": [
    "### INT8 Quantization\n",
    "\n",
    "Now, we'll apply full integer quantization, which converts weights and activations to 8-bit integers. This is one of the most aggressive form of quantization and can result in significant size reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42a935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize the model to int8 (full integer quantization)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.float32  # Keep input as float32 for easier evaluation\n",
    "converter.inference_output_type = tf.float32  # Keep output as float32\n",
    "tflite_int8_model = converter.convert()\n",
    "\n",
    "# Save the int8 quantized model\n",
    "with open('quantized_int8_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_int8_model)\n",
    "\n",
    "# Check the size of the int8 quantized TFLite model\n",
    "tflite_int8_model_size = os.path.getsize('quantized_int8_model.tflite')\n",
    "print(f\"Int8 quantized TFLite model size: {tflite_int8_model_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Size reduction: {(tflite_model_size / tflite_int8_model_size):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bf7502",
   "metadata": {},
   "source": [
    "### Evaluating TFLite Models\n",
    "\n",
    "Now, let's evaluate the performance of our quantized TFLite models to see how quantization affects accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de9856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate TFLite model accuracy\n",
    "def evaluate_tflite_model(tflite_model_path, x_test, y_test):\n",
    "    \"\"\"Evaluate a TFLite model on the test dataset.\"\"\"\n",
    "    # Load TFLite model\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    print(f\"\\nEvaluating {tflite_model_path}\")\n",
    "    print(f\"Input details: {input_details[0]['dtype']}, shape: {input_details[0]['shape']}\")\n",
    "    print(f\"Output details: {output_details[0]['dtype']}, shape: {output_details[0]['shape']}\")\n",
    "\n",
    "    # Check if the model is quantized\n",
    "    input_scale, input_zero_point = input_details[0]['quantization']\n",
    "    is_quantized = input_scale != 0\n",
    "\n",
    "    # Test model on all test data\n",
    "    correct_predictions = 0\n",
    "    num_samples = len(x_test)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Get test sample and ensure correct shape (add batch dimension)\n",
    "        test_image = x_test[i:i+1].copy()  # Shape becomes (1, 32, 32, 3)\n",
    "        true_label = y_test[i]\n",
    "\n",
    "        # FIX: Handle quantization properly\n",
    "        if is_quantized:\n",
    "            # For quantized models, we need to convert to the expected type\n",
    "            if input_details[0]['dtype'] == np.int8:\n",
    "                test_image = (test_image / input_scale + input_zero_point).astype(np.int8)\n",
    "            elif input_details[0]['dtype'] == np.uint8:\n",
    "                test_image = (test_image / input_scale + input_zero_point).astype(np.uint8)\n",
    "        else:\n",
    "            # For float models, ensure float32\n",
    "            test_image = test_image.astype(np.float32)\n",
    "\n",
    "        # Ensure correct shape\n",
    "        test_image = np.reshape(test_image, input_details[0]['shape'])\n",
    "\n",
    "        # Set the input tensor\n",
    "        interpreter.set_tensor(input_details[0]['index'], test_image)\n",
    "\n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Get the output tensor\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "        # FIX: Handle output properly\n",
    "        if output_details[0]['dtype'] == np.int8:\n",
    "            # Dequantize if necessary\n",
    "            output_scale, output_zero_point = output_details[0]['quantization']\n",
    "            output = output.astype(np.float32)\n",
    "            output = (output - output_zero_point) * output_scale\n",
    "\n",
    "        # Get predicted label (assuming 10 classes)\n",
    "        predicted_label = np.argmax(output[0])\n",
    "\n",
    "        if predicted_label == true_label:\n",
    "            correct_predictions += 1\n",
    "\n",
    "        # Print progress occasionally\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"  Processed {i + 1}/{num_samples} samples...\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / num_samples\n",
    "\n",
    "    # Get model size\n",
    "    model_size = os.path.getsize(tflite_model_path)\n",
    "\n",
    "    print(f\"\\nTFLite Model: {tflite_model_path}\")\n",
    "    print(f\"  Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  Model Size: {model_size / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"  Correct predictions: {correct_predictions}/{num_samples}\")\n",
    "\n",
    "    return {\n",
    "        'model_name': os.path.basename(tflite_model_path),\n",
    "        'accuracy': accuracy * 100,\n",
    "        'model_size_mb': model_size / 1024 / 1024\n",
    "    }\n",
    "\n",
    "# Evaluate TFLite models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING TFLITE MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tflite_original_results = evaluate_tflite_model('original_model.tflite', x_test, y_test)\n",
    "tflite_fp16_results = evaluate_tflite_model('quantized_fp16_model.tflite', x_test, y_test)\n",
    "tflite_int8_results = evaluate_tflite_model('quantized_int8_model.tflite', x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9aaf49",
   "metadata": {},
   "source": [
    "### Comparing Quantization Results\n",
    "Now let's compare all our models to understand the trade-offs between model size and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99a461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all quantization results\n",
    "quantization_results = [\n",
    "    original_results,\n",
    "    tflite_original_results,\n",
    "    tflite_fp16_results,\n",
    "    tflite_int8_results\n",
    "]\n",
    "\n",
    "# Create a dataframe and print results\n",
    "quantization_df = pd.DataFrame(quantization_results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUANTIZATION RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(quantization_df.to_string(index=False))\n",
    "\n",
    "# Plot quantization results\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot accuracy comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "bars1 = plt.bar(quantization_df['model_name'], quantization_df['accuracy'], \n",
    "                color=['blue', 'green', 'orange', 'red'])\n",
    "plt.title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim([0, 105])  # Give some headroom for labels\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "# Plot model size comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "bars2 = plt.bar(quantization_df['model_name'], quantization_df['model_size_mb'],\n",
    "                color=['blue', 'green', 'orange', 'red'])\n",
    "plt.title('Model Size Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Model Size (MB)')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f} MB', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot trade-off between accuracy and model size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create scatter plot with different colors for each model\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "sizes = [100, 100, 100, 100]\n",
    "\n",
    "for i, row in quantization_df.iterrows():\n",
    "    plt.scatter(row['model_size_mb'], row['accuracy'], \n",
    "               s=200, color=colors[i], label=row['model_name'], alpha=0.7)\n",
    "\n",
    "# Add connecting lines to show trade-off\n",
    "plt.plot(quantization_df['model_size_mb'], quantization_df['accuracy'], \n",
    "         'k--', alpha=0.3, label='Trade-off curve')\n",
    "\n",
    "# Annotate points\n",
    "for i, row in quantization_df.iterrows():\n",
    "    plt.annotate(row['model_name'],\n",
    "                (row['model_size_mb'], row['accuracy']),\n",
    "                xytext=(10, 5), textcoords='offset points',\n",
    "                fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.title('Accuracy vs. Model Size Trade-off', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Model Size (MB)')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<25} {'Accuracy (%)':<15} {'Size (MB)':<15} {'Size Reduction':<15}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i, row in quantization_df.iterrows():\n",
    "    if i == 0:\n",
    "        size_reduction = \"0% (baseline)\"\n",
    "    else:\n",
    "        reduction = (1 - row['model_size_mb'] / quantization_df.iloc[0]['model_size_mb']) * 100\n",
    "        size_reduction = f\"{reduction:.1f}%\"\n",
    "    \n",
    "    print(f\"{row['model_name']:<25} {row['accuracy']:<15.2f} {row['model_size_mb']:<15.2f} {size_reduction:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e63a64",
   "metadata": {},
   "source": [
    "### Quantization Summary\n",
    "\n",
    "In this section, we've applied different quantization techniques to our CNN model:\n",
    "\n",
    "1. **TensorFlow Lite Conversion**: Converting to TFLite format without quantization\n",
    "2. **Float16 Quantization**: Reducing weight precision from 32-bit to 16-bit floating point\n",
    "3. **INT8 Quantization**: Full integer quantization with 8-bit weights and activations\n",
    "\n",
    "We've observed how each technique affects model size and accuracy. The results show that quantization can significantly reduce model size with minimal impact on accuracy, making it an effective technique for deploying models to resource-constrained environments like mobile devices and edge hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eba4848",
   "metadata": {},
   "source": [
    "## PART-3: MODEL PRUNING\n",
    "\n",
    "In this section, we'll explore model pruning and its effects on model size and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow_model_optimization again to ensure it's available\n",
    "try:\n",
    "  import tensorflow_model_optimization as tfmot\n",
    "  from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "  from tensorflow.keras.models import Model\n",
    "except:\n",
    "  !pip install tensorflow-model-optimization\n",
    "  import tensorflow_model_optimization as tfmot\n",
    "  from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "  from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52f0735",
   "metadata": {},
   "source": [
    "### Manual Pruning Implementation\n",
    "\n",
    "We'll implement a manual pruning approach that removes weights based on their magnitude. This is a common approach where weights with small absolute values are considered less important and set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d238565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a prunable model\n",
    "def create_prunable_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"Create a functional CNN model for pruning compatibility.\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Define a function to manually prune weights based on magnitude\n",
    "def prune_weights(model, sparsity=0.5):\n",
    "    \"\"\"\n",
    "    Prune model weights based on magnitude.\n",
    "\n",
    "    Args:\n",
    "        model: Keras model to prune\n",
    "        sparsity: Target sparsity level (fraction of weights to be pruned)\n",
    "\n",
    "    Returns:\n",
    "        Pruned model\n",
    "    \"\"\"\n",
    "    print(f\"\\nApplying magnitude-based pruning with {sparsity:.0%} sparsity...\")\n",
    "\n",
    "    pruned_model = tf.keras.models.clone_model(model)\n",
    "    pruned_model.set_weights(model.get_weights())\n",
    "\n",
    "    # Prune eligible layers (Conv and Dense)\n",
    "    for i, layer in enumerate(pruned_model.layers):\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            weights = layer.get_weights()\n",
    "\n",
    "            # Only prune the weight matrix, not the bias\n",
    "            weight_matrix = weights[0]\n",
    "\n",
    "            # Flatten the weight matrix to identify the threshold\n",
    "            flat_weights = weight_matrix.flatten()\n",
    "            abs_weights = np.abs(flat_weights)\n",
    "\n",
    "            # Calculate the threshold value based on the sparsity level\n",
    "            k = int(flat_weights.size * sparsity)\n",
    "            if k > 0:\n",
    "                threshold = np.partition(abs_weights, k)[k]\n",
    "\n",
    "                # Create a mask for weights with magnitude below threshold\n",
    "                mask = np.abs(weight_matrix) > threshold\n",
    "\n",
    "                # Apply the mask to the weights\n",
    "                pruned_weights = weight_matrix * mask\n",
    "\n",
    "                # Update the weights\n",
    "                weights[0] = pruned_weights\n",
    "                layer.set_weights(weights)\n",
    "\n",
    "                non_zero = np.count_nonzero(pruned_weights)\n",
    "                total = pruned_weights.size\n",
    "                print(f\"  Layer {i} ({layer.name}): {non_zero}/{total} weights retained ({non_zero/total:.2%})\")\n",
    "\n",
    "    # Recompile the model with the same settings\n",
    "    pruned_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',  # Changed to sparse\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "# Prepare validation data from training data\n",
    "# Use 10% of training data for validation\n",
    "val_split = 0.1\n",
    "val_size = int(len(x_train) * val_split)\n",
    "\n",
    "# Shuffle indices\n",
    "indices = np.random.permutation(len(x_train))\n",
    "val_indices = indices[:val_size]\n",
    "train_indices = indices[val_size:]\n",
    "\n",
    "x_train_subset = x_train[train_indices]\n",
    "y_train_subset = y_train[train_indices]\n",
    "x_val = x_train[val_indices]\n",
    "y_val = y_train[val_indices]\n",
    "\n",
    "print(f\"Training samples: {len(x_train_subset)}\")\n",
    "print(f\"Validation samples: {len(x_val)}\")\n",
    "\n",
    "# Create and train model using the accelerator strategy\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING MODEL FOR PRUNING ON ACCELERATOR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "with strategy.scope():\n",
    "    # Create a new model for pruning within the strategy scope\n",
    "    input_shape = (32, 32, 3)\n",
    "    num_classes = 10\n",
    "    cnn_model_for_pruning = create_prunable_cnn_model(input_shape, num_classes)\n",
    "    \n",
    "    # First, train the model before pruning\n",
    "    print(\"\\nTraining model before pruning...\")\n",
    "    history = cnn_model_for_pruning.fit(\n",
    "        x_train_subset, y_train_subset,\n",
    "        batch_size=128,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=(x_val, y_val),\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Define evaluation function\n",
    "def prune_evaluate_model(model, x_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"Evaluate model accuracy and estimated size after pruning.\"\"\"\n",
    "    total_size = 0\n",
    "    nonzero_size = 0\n",
    "\n",
    "    for weight in model.weights:\n",
    "        w_np = weight.numpy()\n",
    "        total_size += w_np.nbytes\n",
    "        nonzero_size += np.count_nonzero(w_np) * w_np.itemsize\n",
    "\n",
    "    # Accuracy\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "    print(f\"\\n{model_name} Evaluation:\")\n",
    "    print(f\"  Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"  Original Size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"  Estimated Pruned Size: {nonzero_size / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"  Size Reduction: {(1 - nonzero_size / total_size) * 100:.2f}%\")\n",
    "\n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy * 100,\n",
    "        'original_size_mb': total_size / 1024 / 1024,\n",
    "        'pruned_size_mb': nonzero_size / 1024 / 1024,\n",
    "        'estimated_sparsity': 1 - (nonzero_size / total_size)\n",
    "    }\n",
    "\n",
    "# Evaluate before pruning\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING BEFORE PRUNING\")\n",
    "print(\"=\"*60)\n",
    "pre_prune_results = prune_evaluate_model(\n",
    "    cnn_model_for_pruning, \n",
    "    x_test, y_test, \n",
    "    \"Model Before Pruning\"\n",
    ")\n",
    "\n",
    "# Apply pruning with different sparsity levels\n",
    "sparsity_levels = [0.3, 0.5, 0.7, 0.9]\n",
    "pruning_results = []\n",
    "\n",
    "for sparsity in sparsity_levels:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"PRUNING WITH {sparsity:.0%} SPARSITY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Prune the model\n",
    "    pruned_model = prune_weights(cnn_model_for_pruning, sparsity=sparsity)\n",
    "    \n",
    "    # Evaluate pruned model\n",
    "    results = prune_evaluate_model(\n",
    "        pruned_model, \n",
    "        x_test, y_test, \n",
    "        f\"Model {sparsity:.0%} Pruned\"\n",
    "    )\n",
    "    pruning_results.append(results)\n",
    "\n",
    "# Visualize pruning results\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Combine all results\n",
    "all_results = [pre_prune_results] + pruning_results\n",
    "\n",
    "# Create DataFrame\n",
    "pruning_df = pd.DataFrame(all_results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRUNING RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(pruning_df.to_string(index=False))\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Accuracy vs Sparsity\n",
    "sparsity_values = [0] + sparsity_levels\n",
    "accuracy_values = [pre_prune_results['accuracy']] + [r['accuracy'] for r in pruning_results]\n",
    "\n",
    "ax1.plot(sparsity_values, accuracy_values, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Sparsity Level', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax1.set_title('Accuracy vs Pruning Sparsity', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(sparsity_values)\n",
    "\n",
    "# Plot 2: Size Reduction\n",
    "size_reduction = [0] + [r['estimated_sparsity'] * 100 for r in pruning_results]\n",
    "ax2.bar([f\"{s:.0%}\" for s in sparsity_values], size_reduction, \n",
    "        color=['blue', 'green', 'orange', 'red', 'purple'])\n",
    "ax2.set_xlabel('Sparsity Level', fontsize=12)\n",
    "ax2.set_ylabel('Size Reduction (%)', fontsize=12)\n",
    "ax2.set_title('Model Size Reduction', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED PRUNING COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<20} {'Accuracy (%)':<15} {'Size (MB)':<15} {'Reduction':<15}\")\n",
    "print(\"-\"*65)\n",
    "\n",
    "for i, row in pruning_df.iterrows():\n",
    "    if i == 0:\n",
    "        reduction = \"0% (baseline)\"\n",
    "    else:\n",
    "        reduction = f\"{row['estimated_sparsity']*100:.1f}%\"\n",
    "    print(f\"{row['model_name']:<20} {row['accuracy']:<15.2f} {row['pruned_size_mb']:<15.2f} {reduction:<15}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
