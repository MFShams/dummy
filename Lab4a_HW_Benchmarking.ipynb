{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20cbba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the needed libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models, losses, optimizers, metrics\n",
    "import os\n",
    "\n",
    "# Check for accelerator (GPU or TPU)\n",
    "accelerator_found = False\n",
    "using_tpu = False\n",
    "strategy = None\n",
    "\n",
    "# Check for TPU first\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # Will detect TPU in Colab\n",
    "    print(f\"TPU detected: {tpu.cluster_spec().as_dict()}\")\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    using_tpu = True\n",
    "    accelerator_found = True\n",
    "    print(f\"TPU initialized with {strategy.num_replicas_in_sync} replicas\")\n",
    "except (ValueError, AttributeError, ImportError):\n",
    "    # Check for GPU\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        accelerator_found = True\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "        print(f\"GPU detected: {gpus[0].name}\")\n",
    "        # Set memory growth to avoid OOM\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    else:\n",
    "        # Use CPU\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "        print(\"No accelerator detected, using CPU\")\n",
    "\n",
    "if not accelerator_found:\n",
    "    print(\"Warning: No accelerator (GPU or TPU) detected! Using CPU.\")\n",
    "\n",
    "# CREATE A CNN MODEL\n",
    "def create_cnn_model(num_classes=10):\n",
    "    \"\"\"Create a simple CNN model for classification.\"\"\"\n",
    "    model = models.Sequential([\n",
    "        # First convolutional block\n",
    "        layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_and_prepare_cifar10(batch_size=32, train_fraction=0.1, val_fraction=0.1):\n",
    "    \"\"\"\n",
    "    Load and prepare a small portion of CIFAR-10 dataset.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Batch size for datasets\n",
    "        train_fraction: Fraction of training data to use (0.0 to 1.0)\n",
    "        val_fraction: Fraction of training data to use for validation (0.0 to 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        train_ds, val_ds, test_ds: tf.data.Dataset objects\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define preprocessing function\n",
    "    def preprocess(image, label):\n",
    "        # Convert image to float32 and normalize to [0,1]\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "        # Normalize with CIFAR-10 mean and std\n",
    "        mean = tf.constant([0.4914, 0.4822, 0.4465])\n",
    "        std = tf.constant([0.2470, 0.2435, 0.2616])\n",
    "        image = (image - mean) / std\n",
    "        return image, label\n",
    "    \n",
    "    # Load full CIFAR-10 dataset\n",
    "    train_ds_full, test_ds = tfds.load(\n",
    "        'cifar10',\n",
    "        split=['train', 'test'],\n",
    "        as_supervised=True,\n",
    "        batch_size=-1  # Load all data at once for splitting\n",
    "    )\n",
    "    \n",
    "    # Convert to numpy for easier subset creation\n",
    "    train_images, train_labels = tfds.as_numpy(train_ds_full)\n",
    "    test_images, test_labels = tfds.as_numpy(test_ds)\n",
    "    \n",
    "    # Calculate sizes for training subset\n",
    "    total_train = len(train_images)\n",
    "    train_size = int(total_train * train_fraction)\n",
    "    val_size = int(total_train * val_fraction)\n",
    "    \n",
    "    # Randomly shuffle indices\n",
    "    indices = np.random.permutation(total_train)\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:train_size + val_size]\n",
    "    \n",
    "    # Create subsets\n",
    "    train_images_subset = train_images[train_indices]\n",
    "    train_labels_subset = train_labels[train_indices]\n",
    "    val_images = train_images[val_indices]\n",
    "    val_labels = train_labels[val_indices]\n",
    "    \n",
    "    print(f\"Training samples: {len(train_images_subset)}\")\n",
    "    print(f\"Validation samples: {len(val_images)}\")\n",
    "    print(f\"Test samples: {len(test_images)}\")\n",
    "    \n",
    "    # Create tf.data.Dataset objects\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_images_subset, train_labels_subset))\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "    \n",
    "    # Apply preprocessing and batching\n",
    "    train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Batch and prefetch\n",
    "    train_ds = train_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "def train_with_strategy(strategy, train_ds, val_ds, num_epochs=5):\n",
    "    \"\"\"Train the model using the specified distribution strategy.\"\"\"\n",
    "    \n",
    "    print(f\"Training with strategy: {strategy}\")\n",
    "    \n",
    "    with strategy.scope():\n",
    "        # Create model\n",
    "        model = create_cnn_model(num_classes=10)\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "            loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Print model summary\n",
    "        model.summary()\n",
    "    \n",
    "    # Initialize timing variables\n",
    "    total_training_time = 0\n",
    "    total_samples_processed = 0\n",
    "    epoch_metrics = []\n",
    "    \n",
    "    # Custom training loop for timing\n",
    "    for epoch in range(num_epochs):\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train for one epoch\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            epochs=1,\n",
    "            validation_data=val_ds,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # End timing\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate samples processed\n",
    "        samples_processed = sum(1 for _ in train_ds.unbatch().batch(1))\n",
    "        \n",
    "        # Update totals\n",
    "        total_training_time += epoch_time\n",
    "        total_samples_processed += samples_processed\n",
    "        \n",
    "        # Calculate samples per second\n",
    "        samples_per_second = samples_processed / epoch_time\n",
    "        \n",
    "        # Store metrics\n",
    "        epoch_metrics.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': history.history['loss'][0],\n",
    "            'train_acc': history.history['accuracy'][0],\n",
    "            'val_loss': history.history['val_loss'][0],\n",
    "            'val_acc': history.history['val_accuracy'][0],\n",
    "            'epoch_time': epoch_time,\n",
    "            'samples_per_second': samples_per_second\n",
    "        })\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {history.history[\"loss\"][0]:.4f}, Train Acc: {history.history[\"accuracy\"][0]:.4f}')\n",
    "        print(f'Val Loss: {history.history[\"val_loss\"][0]:.4f}, Val Acc: {history.history[\"val_accuracy\"][0]:.4f}')\n",
    "        print(f'Epoch Time: {epoch_time:.2f}s, Samples/sec: {samples_per_second:.2f}')\n",
    "        print('-' * 50)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    avg_samples_per_second = total_samples_processed / total_training_time\n",
    "    final_val_acc = epoch_metrics[-1]['val_acc']\n",
    "    \n",
    "    print(f\"\\nOverall Training Summary:\")\n",
    "    print(f\"Total Training Time: {total_training_time:.2f}s\")\n",
    "    print(f\"Average Samples/second: {avg_samples_per_second:.2f}\")\n",
    "    print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "    \n",
    "    return model, total_training_time, avg_samples_per_second, final_val_acc\n",
    "\n",
    "def test_model(model, test_ds):\n",
    "    \"\"\"Test the model.\"\"\"\n",
    "    start_time = time.time()\n",
    "    test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
    "    test_time = time.time() - start_time\n",
    "    \n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "    print(f'Test Time: {test_time:.2f}s')\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def profile_inference(models_dict, test_ds, batch_sizes=[1, 32, 64], num_iterations=100):\n",
    "    \"\"\"\n",
    "    Profile inference performance on CPU and accelerator for different batch sizes.\n",
    "    \n",
    "    Args:\n",
    "        models_dict: Dictionary with keys 'cpu' and 'accelerator' containing trained models\n",
    "        test_ds: tf.data.Dataset for test dataset\n",
    "        batch_sizes: List of batch sizes to profile\n",
    "        num_iterations: Number of inference iterations for accurate timing\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with profiling results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'cpu': {},\n",
    "        'accelerator': {}\n",
    "    }\n",
    "    \n",
    "    # Get a fixed batch from test dataset\n",
    "    for batch in test_ds.take(1):\n",
    "        fixed_inputs, _ = batch\n",
    "    \n",
    "    for device_name, model in models_dict.items():\n",
    "        print(f\"\\nProfiling inference on {device_name.upper()}...\")\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            # Prepare batch\n",
    "            if batch_size <= fixed_inputs.shape[0]:\n",
    "                inputs = fixed_inputs[:batch_size]\n",
    "            else:\n",
    "                # Repeat to create larger batch\n",
    "                repeats = (batch_size + fixed_inputs.shape[0] - 1) // fixed_inputs.shape[0]\n",
    "                inputs = tf.tile(fixed_inputs, [repeats, 1, 1, 1])[:batch_size]\n",
    "            \n",
    "            # Warm-up\n",
    "            for _ in range(10):\n",
    "                _ = model.predict(inputs, verbose=0)\n",
    "            \n",
    "            # Timed inference\n",
    "            start_time = time.time()\n",
    "            for _ in range(num_iterations):\n",
    "                _ = model.predict(inputs, verbose=0)\n",
    "            \n",
    "            # Ensure all operations are complete\n",
    "            if tf.config.list_physical_devices('GPU'):\n",
    "                tf.keras.backend.clear_session()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            total_time_seconds = end_time - start_time\n",
    "            total_time_ms = total_time_seconds * 1000\n",
    "            \n",
    "            # Calculate metrics\n",
    "            avg_time_per_batch_ms = total_time_ms / num_iterations\n",
    "            samples_per_second = (batch_size * num_iterations) / total_time_seconds\n",
    "            \n",
    "            results[device_name][batch_size] = {\n",
    "                'avg_inference_time_ms': avg_time_per_batch_ms,\n",
    "                'samples_per_second': samples_per_second,\n",
    "                'total_time_seconds': total_time_seconds,\n",
    "                'num_iterations': num_iterations\n",
    "            }\n",
    "            \n",
    "            print(f\"  Batch size {batch_size}:\")\n",
    "            print(f\"    Avg inference time: {avg_time_per_batch_ms:.2f} ms\")\n",
    "            print(f\"    Samples/second: {samples_per_second:.2f}\")\n",
    "    \n",
    "    # Print comparison summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INFERENCE PROFILING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Batch Size':<12} {'Metric':<20} {'CPU':<15} {'Accelerator':<15} {'Speedup':<10}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        cpu_time = results['cpu'][batch_size]['avg_inference_time_ms']\n",
    "        acc_time = results['accelerator'][batch_size]['avg_inference_time_ms']\n",
    "        speedup_time = cpu_time / acc_time if acc_time > 0 else float('inf')\n",
    "        \n",
    "        cpu_throughput = results['cpu'][batch_size]['samples_per_second']\n",
    "        acc_throughput = results['accelerator'][batch_size]['samples_per_second']\n",
    "        speedup_throughput = acc_throughput / cpu_throughput if cpu_throughput > 0 else float('inf')\n",
    "        \n",
    "        print(f\"{batch_size:<12} {'Time (ms)':<20} {cpu_time:<15.2f} {acc_time:<15.2f} {speedup_time:<10.2f}x\")\n",
    "        print(f\"{batch_size:<12} {'Samples/sec':<20} {cpu_throughput:<15.2f} {acc_throughput:<15.2f} {speedup_throughput:<10.2f}x\")\n",
    "        print(\"-\"*60)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_inference_results(profile_results):\n",
    "    \"\"\"\n",
    "    Plot inference profiling results comparing CPU vs Accelerator.\n",
    "    \n",
    "    Args:\n",
    "        profile_results: Dictionary returned by profile_inference() function\n",
    "    \"\"\"\n",
    "    batch_sizes = list(profile_results['cpu'].keys())\n",
    "    \n",
    "    # Extract data\n",
    "    cpu_times = [profile_results['cpu'][bs]['avg_inference_time_ms'] for bs in batch_sizes]\n",
    "    acc_times = [profile_results['accelerator'][bs]['avg_inference_time_ms'] for bs in batch_sizes]\n",
    "    \n",
    "    cpu_throughput = [profile_results['cpu'][bs]['samples_per_second'] for bs in batch_sizes]\n",
    "    acc_throughput = [profile_results['accelerator'][bs]['samples_per_second'] for bs in batch_sizes]\n",
    "    \n",
    "    # Calculate speedups\n",
    "    time_speedups = [cpu_times[i] / acc_times[i] for i in range(len(batch_sizes))]\n",
    "    throughput_speedups = [acc_throughput[i] / cpu_throughput[i] for i in range(len(batch_sizes))]\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.suptitle('Inference Performance: CPU vs Accelerator', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Inference Time (bar chart)\n",
    "    x = np.arange(len(batch_sizes))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, cpu_times, width, label='CPU', color='skyblue', edgecolor='navy')\n",
    "    bars2 = ax1.bar(x + width/2, acc_times, width, label='Accelerator', color='lightcoral', edgecolor='darkred')\n",
    "    \n",
    "    ax1.set_xlabel('Batch Size', fontsize=12)\n",
    "    ax1.set_ylabel('Inference Time (ms)', fontsize=12)\n",
    "    ax1.set_title('Average Inference Time per Batch', fontsize=14)\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([f'BS={bs}' for bs in batch_sizes])\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Plot 2: Throughput (bar chart)\n",
    "    bars1 = ax2.bar(x - width/2, cpu_throughput, width, label='CPU', color='skyblue', edgecolor='navy')\n",
    "    bars2 = ax2.bar(x + width/2, acc_throughput, width, label='Accelerator', color='lightcoral', edgecolor='darkred')\n",
    "    \n",
    "    ax2.set_xlabel('Batch Size', fontsize=12)\n",
    "    ax2.set_ylabel('Samples/Second', fontsize=12)\n",
    "    ax2.set_title('Throughput', fontsize=14)\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels([f'BS={bs}' for bs in batch_sizes])\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.0f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed summary table\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DETAILED INFERENCE PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Batch Size':<12} {'Metric':<20} {'CPU':<15} {'Accelerator':<15} {'Speedup':<10}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for i, bs in enumerate(batch_sizes):\n",
    "        print(f\"{bs:<12} {'Time (ms)':<20} {cpu_times[i]:<15.2f} {acc_times[i]:<15.2f} {time_speedups[i]:<10.2f}x\")\n",
    "        print(f\"{bs:<12} {'Samples/sec':<20} {cpu_throughput[i]:<15.0f} {acc_throughput[i]:<15.0f} {throughput_speedups[i]:<10.2f}x\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "# Main execution\n",
    "print(\"Loading and preparing CIFAR-10 dataset...\")\n",
    "train_ds, val_ds, test_ds = load_and_prepare_cifar10(\n",
    "    batch_size=32,\n",
    "    train_fraction=0.1,\n",
    "    val_fraction=0.1\n",
    ")\n",
    "\n",
    "# Print CIFAR-10 class names\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "print(f\"Classes: {classes}\")\n",
    "\n",
    "# Train on CPU\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ON CPU\")\n",
    "print(\"=\"*60)\n",
    "cpu_strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "cpu_model, cpu_time, cpu_samples_per_sec, cpu_val_acc = train_with_strategy(\n",
    "    cpu_strategy, train_ds, val_ds, num_epochs=5\n",
    ")\n",
    "\n",
    "# Test CPU model\n",
    "print(\"\\nTesting CPU model:\")\n",
    "cpu_test_loss, cpu_test_acc = test_model(cpu_model, test_ds)\n",
    "\n",
    "# Train on accelerator (GPU or TPU)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ON ACCELERATOR\")\n",
    "print(\"=\"*60)\n",
    "accelerator_model, accelerator_time, accelerator_samples_per_sec, accelerator_val_acc = train_with_strategy(\n",
    "    strategy, train_ds, val_ds, num_epochs=5\n",
    ")\n",
    "\n",
    "# Test accelerator model\n",
    "print(\"\\nTesting Accelerator model:\")\n",
    "accelerator_test_loss, accelerator_test_acc = test_model(accelerator_model, test_ds)\n",
    "\n",
    "# Profile inference\n",
    "models_for_profiling = {\n",
    "    'cpu': cpu_model,\n",
    "    'accelerator': accelerator_model\n",
    "}\n",
    "profile_results = profile_inference(models_for_profiling, test_ds, batch_sizes=[1, 32, 64], num_iterations=100)\n",
    "\n",
    "# Plot results\n",
    "plot_inference_results(profile_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d8b6f",
   "metadata": {},
   "source": [
    "# Lab 4a: Hardware Benchmarking (HW Accelerators Prespective)\n",
    "## Hardware for Machine Learning Course\n",
    "\n",
    "This notebook is to benchmark different HWs from their architecture prespective.\n",
    "Part-1 covers:\n",
    "1. Environment setup, Model and dataset preparation\n",
    "2. CPU performance benchmarking\n",
    "3. GPU performance benchmarking\n",
    "\n",
    "Part-2 covers:\n",
    "1. \n",
    "\n",
    "The lab will explore how a neural network model perform across different hardware platforms and how they can be optimized for specific deployment scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4612fb",
   "metadata": {},
   "source": [
    "## PART-1: LATENCY BENCHMARKING\n",
    "### PART 1-1: ENVIRONMENT SETUP\n",
    "\n",
    "First, we'll set up our environment by importing necessary libraries and checking available hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa0707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the needed libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83dbfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for accelerator (GPU or TPU)\n",
    "accelerator_found = False\n",
    "using_tpu = False\n",
    "\n",
    "# Check for GPU\n",
    "if torch.cuda.is_available():\n",
    "    accelerator_found = True\n",
    "    cuda_device = torch.device('cuda')\n",
    "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    try:\n",
    "        import torch_xla\n",
    "        import torch_xla.core.xla_model as xm\n",
    "        import torch_xla.distributed.parallel_loader as pl\n",
    "        # Check for TPU\n",
    "        cuda_device = xm.xla_device()\n",
    "        using_tpu = True\n",
    "        accelerator_found = True\n",
    "        print(f\"TPU detected: {cuda_device}\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "if not accelerator_found:\n",
    "    raise RuntimeError(\"No accelerator (GPU or TPU) detected! Please choose a GPU or TPU runtime in Colab.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb70de1",
   "metadata": {},
   "source": [
    "### PART 1-2: MODEL, DATASET PREPARATION, AND BUILDING HELPING FUNCTIONS\n",
    "Now we'll create our model architectures and prepare the CIFAR-10 dataset for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bcb276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A CNN MODEL\n",
    "# PREPARE A SUBSET OF THE CIFAR-10 DATASET\n",
    "class CNNModel(nn.Module):\n",
    "    \"\"\"Simple CNN model for classification.\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 128),  # CIFAR-10 images are 32x32, after two poolings: 8x8\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        # Note: No softmax here as it's included in CrossEntropyLoss\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "def load_and_prepare_cifar10(batch_size=32, train_fraction=0.1, val_fraction=0.1):\n",
    "    \"\"\"\n",
    "    Load and prepare a small portion of CIFAR-10 dataset.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Batch size for dataloaders\n",
    "        train_fraction: Fraction of training data to use (0.0 to 1.0)\n",
    "        val_fraction: Fraction of training data to use for validation (0.0 to 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader: DataLoaders for each dataset split\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define transforms for data preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Converts to [0, 1] range\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))  # CIFAR-10 mean and std\n",
    "    ])\n",
    "    \n",
    "    # Download and load training dataset\n",
    "    full_trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    "    )\n",
    "    \n",
    "    # Download and load test dataset\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform\n",
    "    )\n",
    "    \n",
    "    # Calculate sizes for training subset\n",
    "    total_train = len(full_trainset)\n",
    "    train_size = int(total_train * train_fraction)\n",
    "    val_size = int(total_train * val_fraction)\n",
    "    \n",
    "    # Create indices for random subset\n",
    "    indices = torch.randperm(total_train)\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:train_size + val_size]\n",
    "    \n",
    "    # Create subset datasets\n",
    "    trainset = Subset(full_trainset, train_indices)\n",
    "    valset = Subset(full_trainset, val_indices)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"Training samples: {len(trainset)}\")\n",
    "    print(f\"Validation samples: {len(valset)}\")\n",
    "    print(f\"Test samples: {len(testset)}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35286b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, using_tpu=False):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # For TPU, we need to handle the loader differently\n",
    "    if using_tpu:\n",
    "        import torch_xla.distributed.parallel_loader as pl\n",
    "        train_loader = pl.MpDeviceLoader(train_loader, device)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        \n",
    "        # For TPU, we need to use xm.optimizer_step\n",
    "        if using_tpu:\n",
    "            import torch_xla.core.xla_model as xm\n",
    "            xm.optimizer_step(optimizer)\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def train(train_loader, val_loader, criterion, device, num_epochs=5):\n",
    "    # Training loop\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Determine if we're using TPU\n",
    "    using_tpu = device.type == 'xla'\n",
    "    \n",
    "    # Create models\n",
    "    model = CNNModel(num_classes=10).to(device)\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Initialize timing variables\n",
    "    total_training_time = 0\n",
    "    total_samples_processed = 0\n",
    "    epoch_metrics = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Start timing for this epoch\n",
    "        if device.type == 'cuda':\n",
    "            # CUDA events for GPU timing\n",
    "            start_event = torch.cuda.Event(enable_timing=True)\n",
    "            end_event = torch.cuda.Event(enable_timing=True)\n",
    "            start_event.record()\n",
    "        elif using_tpu:\n",
    "            # For TPU, use Python time\n",
    "            start_time = time.time()\n",
    "        else:\n",
    "            # CPU timing\n",
    "            start_time = time.time()\n",
    "        \n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, using_tpu)\n",
    "        \n",
    "        # End timing for this epoch\n",
    "        if device.type == 'cuda':\n",
    "            end_event.record()\n",
    "            torch.cuda.synchronize()\n",
    "            epoch_time = start_event.elapsed_time(end_event) / 1000  # Convert to seconds\n",
    "        else:\n",
    "            epoch_time = time.time() - start_time\n",
    "        \n",
    "        # Update totals\n",
    "        total_training_time += epoch_time\n",
    "        total_samples_processed += len(train_loader.dataset)\n",
    "        \n",
    "        # Calculate samples per second for this epoch\n",
    "        samples_per_second = len(train_loader.dataset) / epoch_time\n",
    "        \n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device, using_tpu)\n",
    "        \n",
    "        # Store epoch metrics\n",
    "        epoch_metrics.append({\n",
    "            'epoch': epoch+1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'epoch_time': epoch_time,\n",
    "            'samples_per_second': samples_per_second\n",
    "        })\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        print(f'Epoch Time: {epoch_time:.2f}s, Samples/sec: {samples_per_second:.2f}')\n",
    "        print('-' * 50)\n",
    "        \n",
    "    # Calculate overall metrics\n",
    "    avg_samples_per_second = total_samples_processed / total_training_time\n",
    "    final_val_acc = epoch_metrics[-1]['val_acc']  # Last epoch's validation accuracy\n",
    "    \n",
    "    print(f\"\\nOverall Training Summary:\")\n",
    "    print(f\"Total Training Time: {total_training_time:.2f}s\")\n",
    "    print(f\"Average Samples/second: {avg_samples_per_second:.2f}\")\n",
    "    print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "    \n",
    "    return model, total_training_time, avg_samples_per_second, final_val_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device, using_tpu=False):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # For TPU, we need to handle the loader differently\n",
    "    if using_tpu:\n",
    "        import torch_xla.distributed.parallel_loader as pl\n",
    "        val_loader = pl.MpDeviceLoader(val_loader, device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    \"\"\"Test the model.\"\"\"\n",
    "    # Determine if we're using TPU\n",
    "    using_tpu = device.type == 'xla'\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss, test_acc = validate(model, test_loader, criterion, device, using_tpu)\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "    return test_loss, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98f68a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64fee23a",
   "metadata": {},
   "source": [
    "### PREPARING BENCHARKING FOR BOTH THE CPU AND THE GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b420b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data (using 20% of training data: 10% for training, 10% for validation)\n",
    "train_loader, val_loader, test_loader = load_and_prepare_cifar10(\n",
    "    batch_size=32, \n",
    "    train_fraction=0.1,  # 10% of training data for training\n",
    "    val_fraction=0.1      # 10% of training data for validation\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Print CIFAR-10 class names for reference\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "            'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "print(f\"Classes: {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b01c21",
   "metadata": {},
   "source": [
    "### PART 1-3: BENCHMARKING THE CPU TRAINING\n",
    "put some words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99123463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BENCHMARK THE EVALUATION ON BS 1 AND 64\n",
    "# THE METRICS ARE LATENCY, SAMPLES/S, EVALUATION ACCURACY \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924370bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the cnn model using cpu\n",
    "cpu_device = torch.device('cpu')\n",
    "cpu_trained_model, cpu_time, cpu_samples_per_sec, cpu_val_acc = train(\n",
    "    train_loader, val_loader, criterion, cpu_device\n",
    ")\n",
    "\n",
    "# Final test\n",
    "cpu_test_loss, cpu_test_acc = test(cpu_trained_model, test_loader, criterion, cpu_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef290f3",
   "metadata": {},
   "source": [
    "### PART 1-4: BENCHMARKING THE ACCELERATOR TRAINING \n",
    "put some words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e4ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BENCHMARK  THE EVALUATION ON BS 1 AND 64\n",
    "# THE METRICS ARE LATENCY, SAMPLES/S, EVALUATION ACCURACY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919d1513",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the cnn model using accelerator (GPU or TPU)\n",
    "accelerator_trained_model, accelerator_time, accelerator_samples_per_sec, accelerator_val_acc = train(\n",
    "    train_loader, val_loader, criterion, cuda_device\n",
    ")\n",
    "\n",
    "# Final test\n",
    "accelerator_test_loss, accelerator_test_acc = test(accelerator_trained_model, test_loader, criterion, cuda_device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758e9500",
   "metadata": {},
   "source": [
    "### PART 1-5: BENCHMARKING THE CPU AND THE ACCELERATOR INFERENCE\n",
    "put some words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f92d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_inference(models_dict, test_loader, batch_sizes=[1, 32, 64], num_iterations=100):\n",
    "    \"\"\"\n",
    "    Profile inference performance on CPU and accelerator for different batch sizes.\n",
    "    \n",
    "    Args:\n",
    "        models_dict: Dictionary with keys 'cpu' and 'accelerator' containing trained models\n",
    "        test_loader: DataLoader for test dataset\n",
    "        batch_sizes: List of batch sizes to profile\n",
    "        num_iterations: Number of inference iterations for accurate timing\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with profiling results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'cpu': {},\n",
    "        'accelerator': {}\n",
    "    }\n",
    "    \n",
    "    # Get a fixed batch from test loader for consistent profiling\n",
    "    data_iter = iter(test_loader)\n",
    "    fixed_inputs, _ = next(data_iter)  # Ignore labels as we only need inputs for inference\n",
    "    \n",
    "    for device_name, model in models_dict.items():\n",
    "        device = next(model.parameters()).device\n",
    "        print(f\"\\nProfiling inference on {device_name.upper()} ({device})...\")\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            # Prepare batch\n",
    "            if batch_size <= len(fixed_inputs):\n",
    "                # Use subset of the fixed batch if possible\n",
    "                inputs = fixed_inputs[:batch_size].to(device)\n",
    "            else:\n",
    "                # Need to create a larger batch by repeating\n",
    "                repeats = (batch_size + len(fixed_inputs) - 1) // len(fixed_inputs)\n",
    "                inputs = fixed_inputs.repeat(repeats, 1, 1, 1)[:batch_size].to(device)\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            # Warm-up\n",
    "            with torch.no_grad():\n",
    "                for _ in range(10):\n",
    "                    _ = model(inputs)\n",
    "            \n",
    "            # Timed inference\n",
    "            if device.type == 'cuda':\n",
    "                # Use CUDA events for GPU timing\n",
    "                start_event = torch.cuda.Event(enable_timing=True)\n",
    "                end_event = torch.cuda.Event(enable_timing=True)\n",
    "                \n",
    "                start_event.record()\n",
    "                with torch.no_grad():\n",
    "                    for _ in range(num_iterations):\n",
    "                        _ = model(inputs)\n",
    "                end_event.record()\n",
    "                \n",
    "                torch.cuda.synchronize()\n",
    "                total_time_ms = start_event.elapsed_time(end_event)  # Returns time in milliseconds\n",
    "                total_time_seconds = total_time_ms / 1000\n",
    "                \n",
    "            else:\n",
    "                # Use time.time() for CPU and TPU\n",
    "                start_time = time.time()\n",
    "                with torch.no_grad():\n",
    "                    for _ in range(num_iterations):\n",
    "                        _ = model(inputs)\n",
    "                \n",
    "                # For TPU, ensure all computations are complete\n",
    "                if device.type == 'xla':\n",
    "                    import torch_xla.core.xla_model as xm\n",
    "                    xm.mark_step()\n",
    "                \n",
    "                end_time = time.time()\n",
    "                total_time_seconds = end_time - start_time\n",
    "                total_time_ms = total_time_seconds * 1000\n",
    "            \n",
    "            # Calculate metrics\n",
    "            avg_time_per_batch_ms = total_time_ms / num_iterations\n",
    "            samples_per_second = (batch_size * num_iterations) / total_time_seconds\n",
    "            \n",
    "            results[device_name][batch_size] = {\n",
    "                'avg_inference_time_ms': avg_time_per_batch_ms,\n",
    "                'samples_per_second': samples_per_second,\n",
    "                'total_time_seconds': total_time_seconds,\n",
    "                'num_iterations': num_iterations\n",
    "            }\n",
    "            \n",
    "            print(f\"  Batch size {batch_size}:\")\n",
    "            print(f\"    Avg inference time: {avg_time_per_batch_ms:.2f} ms\")\n",
    "            print(f\"    Samples/second: {samples_per_second:.2f}\")\n",
    "    \n",
    "    # Print comparison summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INFERENCE PROFILING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Batch Size':<12} {'Metric':<20} {'CPU':<15} {'Accelerator':<15} {'Speedup':<10}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        cpu_time = results['cpu'][batch_size]['avg_inference_time_ms']\n",
    "        acc_time = results['accelerator'][batch_size]['avg_inference_time_ms']\n",
    "        speedup_time = cpu_time / acc_time if acc_time > 0 else float('inf')\n",
    "        \n",
    "        cpu_throughput = results['cpu'][batch_size]['samples_per_second']\n",
    "        acc_throughput = results['accelerator'][batch_size]['samples_per_second']\n",
    "        speedup_throughput = acc_throughput / cpu_throughput if cpu_throughput > 0 else float('inf')\n",
    "        \n",
    "        print(f\"{batch_size:<12} {'Time (ms)':<20} {cpu_time:<15.2f} {acc_time:<15.2f} {speedup_time:<10.2f}x\")\n",
    "        print(f\"{batch_size:<12} {'Samples/sec':<20} {cpu_throughput:<15.2f} {acc_throughput:<15.2f} {speedup_throughput:<10.2f}x\")\n",
    "        print(\"-\"*60)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# After training both models, you can call the profiling method like this:\n",
    "models_for_profiling = {\n",
    "    'cpu': cpu_trained_model,\n",
    "    'accelerator': accelerator_trained_model\n",
    "}\n",
    "profile_results = profile_inference(models_for_profiling, test_loader, batch_sizes=[1, 32, 64], num_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3dea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inference_results(profile_results):\n",
    "    \"\"\"\n",
    "    Plot inference profiling results comparing CPU vs Accelerator.\n",
    "    \n",
    "    Args:\n",
    "        profile_results: Dictionary returned by profile_inference() function\n",
    "    \"\"\"\n",
    "    batch_sizes = list(profile_results['cpu'].keys())\n",
    "    \n",
    "    # Extract data\n",
    "    cpu_times = [profile_results['cpu'][bs]['avg_inference_time_ms'] for bs in batch_sizes]\n",
    "    acc_times = [profile_results['accelerator'][bs]['avg_inference_time_ms'] for bs in batch_sizes]\n",
    "    \n",
    "    cpu_throughput = [profile_results['cpu'][bs]['samples_per_second'] for bs in batch_sizes]\n",
    "    acc_throughput = [profile_results['accelerator'][bs]['samples_per_second'] for bs in batch_sizes]\n",
    "    \n",
    "    # Calculate speedups\n",
    "    time_speedups = [cpu_times[i] / acc_times[i] for i in range(len(batch_sizes))]\n",
    "    throughput_speedups = [acc_throughput[i] / cpu_throughput[i] for i in range(len(batch_sizes))]\n",
    "    \n",
    "    # Create figure with subplots - FIXED: 1 row, 2 columns returns 2 axes in a 1D array\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.suptitle('Inference Performance: CPU vs Accelerator', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Inference Time (bar chart)\n",
    "    ax1 = axes[0]  # FIXED: Changed from axes[0,0] to axes[0]\n",
    "    x = np.arange(len(batch_sizes))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, cpu_times, width, label='CPU', color='skyblue', edgecolor='navy')\n",
    "    bars2 = ax1.bar(x + width/2, acc_times, width, label='Accelerator', color='lightcoral', edgecolor='darkred')\n",
    "    \n",
    "    ax1.set_xlabel('Batch Size', fontsize=12)\n",
    "    ax1.set_ylabel('Inference Time (ms)', fontsize=12)\n",
    "    ax1.set_title('Average Inference Time per Batch', fontsize=14)\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([f'BS={bs}' for bs in batch_sizes])\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Plot 2: Throughput (bar chart)\n",
    "    ax2 = axes[1]  # FIXED: Changed from axes[0,1] to axes[1]\n",
    "    \n",
    "    bars1 = ax2.bar(x - width/2, cpu_throughput, width, label='CPU', color='skyblue', edgecolor='navy')\n",
    "    bars2 = ax2.bar(x + width/2, acc_throughput, width, label='Accelerator', color='lightcoral', edgecolor='darkred')\n",
    "    \n",
    "    ax2.set_xlabel('Batch Size', fontsize=12)\n",
    "    ax2.set_ylabel('Samples/Second', fontsize=12)\n",
    "    ax2.set_title('Throughput', fontsize=14)\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels([f'BS={bs}' for bs in batch_sizes])\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.0f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed summary table\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DETAILED INFERENCE PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Batch Size':<12} {'Metric':<20} {'CPU':<15} {'Accelerator':<15} {'Speedup':<10}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for i, bs in enumerate(batch_sizes):\n",
    "        print(f\"{bs:<12} {'Time (ms)':<20} {cpu_times[i]:<15.2f} {acc_times[i]:<15.2f} {time_speedups[i]:<10.2f}x\")\n",
    "        print(f\"{bs:<12} {'Samples/sec':<20} {cpu_throughput[i]:<15.0f} {acc_throughput[i]:<15.0f} {throughput_speedups[i]:<10.2f}x\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "\n",
    "# After running profile_inference():\n",
    "# profile_results = profile_inference(models_for_profiling, test_loader, batch_sizes=[1, 32, 64], num_iterations=100)\n",
    "plot_inference_results(profile_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb40c1",
   "metadata": {},
   "source": [
    "## Part-2: MODEL QUANTIZATION AND PRUNING\n",
    "In this part we will quantize and prune a model then benchmark its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577c5106",
   "metadata": {},
   "source": [
    "### PART 2-1: BUILDING HELPING FUNCTIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f1b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "build one helping function for INT8 quantization and another for pruning.\n",
    "Do you retrain?? will you use the same old model? will you construct a new one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c846a",
   "metadata": {},
   "source": [
    "### PART 2-2: BENCHMARKING THE GPU WITH INT8 QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7273753",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPEAT THE ACCURACY AND INFERNECE BENCHMARKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784e5b95",
   "metadata": {},
   "source": [
    "### PART 2-3: BENCHMARKING THE GPU WITH STRUCTURAL PRUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c6b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPEAT THE ACCURACY AND INFERNECE BENCHMARKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed146161",
   "metadata": {},
   "source": [
    "## Part-3: DEPLOYMENT FORMAT CONVERSION\n",
    "In this section, we'll convert our models to different formats suitable for various deployment scenarios, such as ONNX for cross-platform compatibility, SavedModel for TensorFlow Serving, and TensorFlow.js for web deployment.\n",
    "\n",
    "\n",
    "WILL WE CONSIDER TENSORFLOW OR PYTORCH???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
